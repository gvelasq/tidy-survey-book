# Understanding survey data files {#c03}

```{r, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

* [Reading survey documentation](#reading-survey-documentation)
* [Best practices for loading data into R](#best-practices-for-loading-data-into-r)
* [Loading survey files into R](#loading-survey-files-into-r)
* [Working with labeled data](#working-with-labeled-data)
* [Missing data]

## Reading survey documentation

The first step in survey analysis is reading the documentation. Survey documentation can vary in its organization, thoroughness, and ease of use. We recommend focusing on key sections when beginning an analysis.

* **Introduction:** The introduction orients us to the survey. Generally, this section provides the background of the project, purpose of the study, and main research questions.
* **Study design:** The study design section describes how the survey was prepared and administered.
* **Sample:** The sample section describes the sampling of the survey: how cases were selected, any sampling error that occurred, and limitations of the sample. This section can contain recommendations on how to use sampling weights. This documentation is critical in successfully running our analysis. We go into more detail in [Chapter 05:Specifying sample designs in srvyr](#c05).

### Searching for public-use data



## Best practices for loading data into R

We recommend a project-based workflow for analysis projects as described in Hadley Wickham and Garrett Grolemund's book, R for Data Science. [define project-based workflow here] Projects help us practice file system discipline, since we put all the files related to a single project in a designated folder. Since all associated files are in a single location, they are easy to find and organize.

The RStudio IDE supports project-based workflows. When we create a project in RStudio, it creates a `.Rproj` file that store settings specific to that project.

The {here} package enables easy file referencing. In a project-based workflow, all paths are relative and, by default, relative to the project’s folder. Use the `here()` function from the here package to build the path when you load or save data.

## Loading survey files into R

Survey files come in different file types depending on the program used to output them. R gives us the flexibility to load datasets regardless of their file extension. Here are examples of common public-use survey file formats:

* Delimiter-separated text files
* Excel files
* `.rda` files
* `.dta` files from Stata
* `.sas` files from SAS
* `.sav` files from SPSS

For uncommon file formats, we recommend searching StackOverflow or similar community sites to learn how to load them into R.

### Loading delimiter-separated files into R

Delimiter-separated files use specific characters to separate values. For example, CSV files are separated by commas. These file types are widely used and we can use many applications to read and write them.

We can read delimiter-separated files using the tidyverse package {readr}. The {readr} package supports the following files with these `readr::read_*()` functions:

* `read_csv()`: Comma-separated values (CSV) files
* `read_tsv()`: Tab-separated values (TSV) files
* `read_delim()`: Delimiter-separated files (CSV and TSV are important special cases)
* `read_fwf()`: Fixed-width files
* `read_table()`: Whitespace-separated files
* `read_log()`: Web log files

```{r}

```

### Loading Excel `.xlsx` files into R

Excel, the spreadsheet software program from Microsoft, is a common file format. 
We can load Excel spreadsheets into R using the {readxl} package. The {readxl} package supports both the legacy `.xls` format and the modern `.xlsx` format. 

```{r}

```

### Loading Stata `.dta`, SAS `.sas`, and SPSS `.sav` files into R

Stata is a popular statistical software program for data manipulation, visualization, statistics, and automated reporting. Stata stores data in the proprietary `.dta` format. This file type can contain labels for variables and values.

The {haven} package from the tidyverse imports a variety of proprietary data formats, including Stata `.dta` files, SAS `.sas` files, and SPSS `.sav` files. Once imported, we can analyze the resulting tibbles in R.

We can explore `haven::read_dat()` options by searching for the help page, `??haven::read_dta`.

```r
read_dta(
  file,
  encoding = NULL,
  col_select = NULL,
  skip = 0,
  n_max = Inf,
  .name_repair = "unique"
)
```

We load our file into R by running `read_dat()` and specifying the arguments as necessary:

```{r}
#| eval = FALSE
library(haven)
read_dta(file = "https://www.stata-press.com/data/r17/smho.dta")
```

SAS `.sas` and SPSS `.sav` files are similar to Stata `.dta` files. They are both proprietary file formats, contain labeled data, and can be imported using {haven}.

## Working with labeled data

SPSS and SAS files often contain labeled values. We can use the `haven::labeled()` function to import labeled numeric or character vectors into R. The goal is to create an intermediate object that we convert into a regular R data frame. We do this by converting the vector into a factor or stripping the labels.

## Missing data

Stata, SPSS, and SAS files each handle missing values in different ways. For SAS and Stata, {haven} provides `tagged` missing values which extend R’s regular `NA` to add a single character label. A `tagged` missing value adds a single character label to R's regular `NA`. These values behave identical to `NA` in regular R operations while preserving the value of the tag.

SPSS’s user-defined values work differently to SAS and Stata. Each column can have either up to three distinct values that are considered as missing, or a range. Haven provides `labeled_spss()` as a subclass of `labeled()` to model these additional user-defined missings.

## Working with missing data
